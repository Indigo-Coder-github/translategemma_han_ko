model:
  name: google/translategemma-12b-it
  dtype: bfloat16

lora:
  rank: 64
  alpha: 64
  dropout: 0.0
  use_rslora: true
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj

training:
  epochs: 3
  lr: 2.0e-4
  batch_size: 4
  gradient_accumulation_steps: 4
  max_length: 2048
  warmup_ratio: 0.05
  weight_decay: 0.01
  save_steps: 500
  eval_steps: 500
  logging_steps: 10

data:
  path: data/splits/hf_dataset
